{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from res_ind_lib import average_over_rp\n",
    "\n",
    "import glob\n",
    "\n",
    "from progress_reporter import progress_reporter\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def wp(data, wt, percentiles,cum=False): \n",
    "\t\"\"\"Compute weighted percentiles. \n",
    "\tIf the weights are equal, this is the same as normal percentiles. \n",
    "\tElements of the C{data} and C{wt} arrays correspond to \n",
    "\teach other and must have equal length (unless C{wt} is C{None}). \n",
    "\n",
    "\t@param data: The data. \n",
    "\t@type data: A L{numpy.ndarray} array or a C{list} of numbers. \n",
    "\t@param wt: How important is a given piece of data. \n",
    "\t@type wt: C{None} or a L{numpy.ndarray} array or a C{list} of numbers. \n",
    "\t\t All the weights must be non-negative and the sum must be \n",
    "\t\t greater than zero. \n",
    "\t@param percentiles: what percentiles to use.  (Not really percentiles, \n",
    "\t\t as the range is 0-1 rather than 0-100.) \n",
    "\t@type percentiles: a C{list} of numbers between 0 and 1. \n",
    "\t@rtype: [ C{float}, ... ] \n",
    "\t@return: the weighted percentiles of the data. \n",
    "\t\"\"\"\n",
    "\tassert numpy.greater_equal(percentiles, 0.0).all(), \"Percentiles less than zero\" \n",
    "\tassert numpy.less_equal(percentiles, 1.0).all(), \"Percentiles greater than one\" \n",
    "\tdata = numpy.asarray(data) \n",
    "\t# data = numpy.reshape(data,(len(data)))\n",
    "\tassert len(data.shape) == 1 \n",
    "\tif wt is None: \n",
    "\t\t wt = numpy.ones(data.shape, numpy.float) \n",
    "\telse: \n",
    "\t\t wt = numpy.asarray(wt, numpy.float) \n",
    "\t\t # wt = numpy.reshape(wt,(len(wt)))\n",
    "\t\t assert wt.shape == data.shape \n",
    "\t\t assert numpy.greater_equal(wt, 0.0).all(), \"Not all weights are non-negative.\" \n",
    "\ti = numpy.argsort(data) \n",
    "\tsd = numpy.take(data, i, axis=0)\n",
    "\tsw = numpy.take(wt, i, axis=0) \n",
    "\taw = numpy.add.accumulate(sw) \n",
    "\tif not aw[-1] > 0: \n",
    "\t\t raise ValueError(\"Nonpositive weight sum\" )\n",
    "\tw = (aw)/aw[-1] \n",
    "\tspots = numpy.searchsorted(w, percentiles) \n",
    "\tif cum:\n",
    "\t\tsd = numpy.add.accumulate(numpy.multiply(sd,sw))\n",
    "\tf = interp1d(w,sd)\n",
    "\treturn f(percentiles)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from tempfile import mkstemp, SpooledTemporaryFile\n",
    "from shutil import move\n",
    "from os import remove, close\n",
    "from  io import StringIO\n",
    "\n",
    "def res_to_csv(file_path, new_path=None):\n",
    "    #Create temp file\n",
    "    new_file =StringIO()\n",
    "    \n",
    "    #actually replaces text line by line\n",
    "    \n",
    "    data = False\n",
    "    \n",
    "\n",
    "    with open(file_path) as old_file:\n",
    "        for line in old_file:\n",
    "            if line.startswith(\"Temporalidad\"): #begining of data\n",
    "                data=True\n",
    "                colindex = line.find(\"Frecuencia\")\n",
    "\n",
    "            if line.startswith(\"Curva\"):\n",
    "                break\n",
    "\n",
    "            if data:   \n",
    "                try:\n",
    "                    new_file.write(line[colindex:].replace(\"NeuN\",\" 0 \"))\n",
    "                except IndexError:\n",
    "                    print(\"one line omited\")    \n",
    "    \n",
    "\n",
    "    \n",
    "    #closes the temporary file\n",
    "    return new_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "return_periods = np.array( [ 0.1,.5,1,1.5, 2,3, 5, 10, 20, 50, 100, 250, 500, 1000, 1500, 2000, 4000, 5000, 30e3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iso2bank = pd.read_csv(\"inputs/iso3_to_wb_name.csv\",index_col=\"iso3\", squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "res_file_list = glob.glob(\"D:/events/*/*.res\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "PML_guessed = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0b9a8d4c9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_file_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "myfile = res_file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for myfile in res_file_list:\n",
    "\n",
    "    #parse filename\n",
    "    path, name = glob.os.path.split(myfile)\n",
    "    hazard = path.split(\"\\\\\")[-1]\n",
    "    try:\n",
    "        country_name = iso2bank[name.split(\"_\")[1]]\n",
    "    except KeyError:\n",
    "        print(\"ignored \" +myfile)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if ((hazard,country_name) in PML_guessed):\n",
    "        continue\n",
    "    \n",
    "    progress_reporter((hazard,country_name))\n",
    "\n",
    "    try:\n",
    "        #open file and parse it to memory file\n",
    "        memory_file = res_to_csv(myfile)\n",
    "        #gets back to begining or IO\n",
    "        memory_file.seek(0)\n",
    "\n",
    "        #read data\n",
    "        data = pd.read_csv(memory_file,sep=\" *\", engine=\"python\",usecols=[\"Frecuencia\",\"EP\"])\n",
    "        data= data.sort_values(by=\"EP\", ascending=False)[[\"EP\",\"Frecuencia\"]]\n",
    "\n",
    "        data[\"rp\"]=1/data.Frecuencia.cumsum()\n",
    "        data.head()\n",
    "\n",
    "        #interpolates RP\n",
    "        series = pd.DataFrame(interp1d(data.rp,data.EP, bounds_error=False)(return_periods), \n",
    "                              index=pd.Index(return_periods,name=\"rp\"))\n",
    "\n",
    "        #scales fore same average capital losses\n",
    "        series *= (data.EP*data.Frecuencia).sum()/ average_over_rp(series.stack()).squeeze()\n",
    "\n",
    "        PML_guessed[(hazard,country_name)]=series.squeeze()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "PML_guessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=PML_guessed.copy()\n",
    "df.columns=pd.MultiIndex.from_tuples(PML_guessed.columns)\n",
    "df=df.stack().stack()\n",
    "df.index.names=[\"rp\",\"country\",\"hazard\"]\n",
    "df.swaplevel(0,1).swaplevel(1,2).sort_index().to_csv(\"intermediate/capital_losses_from_GAR_events.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_over_rp(df).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum=0\n",
    "\n",
    "for myfile in res_file_list:\n",
    "\n",
    "    #parse filename\n",
    "    path, name = glob.os.path.split(myfile)\n",
    "    hazard = path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    if hazard !=\"surge\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        #open file and parse it to memory file\n",
    "        memory_file = res_to_csv(myfile)\n",
    "        #gets back to begining or IO\n",
    "        memory_file.seek(0)\n",
    "\n",
    "        #read data\n",
    "        data = pd.read_csv(memory_file,sep=\" *\", engine=\"python\",usecols=[\"Frecuencia\",\"EP\"])\n",
    "        data= data.sort_values(by=\"EP\", ascending=False)[[\"EP\",\"Frecuencia\"]]   \n",
    "\n",
    "        #scales fore same average capital losses\n",
    "        sum+= (data.EP*data.Frecuencia).sum()\n",
    "    except:\n",
    "        print(\"passing \"+myfile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "49788 + 247608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = \"D:/events\\wind\\africa_REU_Wd_Total.res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path, name = glob.os.path.split(myfile)\n",
    "hazard = path.split(\"\\\\\")[-1]\n",
    "try:\n",
    "    country_name = iso2bank[name.split(\"_\")[1]]\n",
    "except KeyError:\n",
    "    print(\"ignored \" +myfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memory_file = res_to_csv(myfile)\n",
    "#gets back to begining or IO\n",
    "memory_file.seek(0)\n",
    "\n",
    "#read data\n",
    "data = pd.read_csv(memory_file,sep=\" *\", engine=\"python\",usecols=[\"Frecuencia\",\"EP\"])\n",
    "data= data.sort_values(by=\"EP\", ascending=False)[[\"EP\",\"Frecuencia\"]]\n",
    "\n",
    "data[\"rp\"]=1/data.Frecuencia.cumsum()\n",
    "data.head()\n",
    "\n",
    "#interpolates RP\n",
    "series = pd.DataFrame(interp1d(data.rp,data.EP, bounds_error=False)(return_periods), \n",
    "                      index=pd.Index(return_periods,name=\"rp\"))\n",
    "\n",
    "#scales fore same average capital losses\n",
    "series *= (data.EP*data.Frecuencia).sum()/ average_over_rp(series.stack()).squeeze()\n",
    "\n",
    "PML_guessed[(hazard,country_name)]=series.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
